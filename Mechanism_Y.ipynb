{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63312d91-cf45-4307-af21-172ff7bc67cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Schema defined.\n✅ Reading stream from s3a://devdolphines-task-transaction-landing-krishna/\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, TimestampType\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Define the schema for the incoming transaction data.\n",
    "transaction_schema = StructType([\n",
    "    StructField(\"transaction_id\", StringType(), True),\n",
    "    StructField(\"customer_id\", StringType(), True),\n",
    "    StructField(\"customer_name\", StringType(), True),\n",
    "    StructField(\"merchant_id\", StringType(), True),\n",
    "    StructField(\"transaction_amount\", DoubleType(), True),\n",
    "    StructField(\"transaction_type\", StringType(), True),\n",
    "    StructField(\"gender\", StringType(), True),\n",
    "    StructField(\"timestamp\", TimestampType(), True)\n",
    "])\n",
    "print(\"✅ Schema defined.\")\n",
    "\n",
    "# Point to your S3 bucket\n",
    "S3_LANDING_BUCKET = \"devdolphines-task-transaction-landing-krishna\" \n",
    "s3_path = f\"s3a://{S3_LANDING_BUCKET}/\"\n",
    "\n",
    "# This command tells Spark to monitor the S3 path for new CSV files.\n",
    "# The S3 access is handled by the cluster configuration you set up in Step 1.\n",
    "raw_transactions_df = spark.readStream \\\n",
    "    .format(\"csv\") \\\n",
    "    .schema(transaction_schema) \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .load(s3_path)\n",
    "\n",
    "print(f\"✅ Reading stream from {s3_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73c8b249-2a34-4df2-9837-dbaaf1f90db6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CustomerImportance.csv loaded successfully.\n✅ PatId2 (CHILD) detection stream started.\n✅ PatId3 (DEI-NEEDED) detection stream started.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# --- Load Supplementary Data ---\n",
    "# Load the CustomerImportance data from DBFS\n",
    "importance_file_path = \"/FileStore/tables/CustomerImportance.csv\"\n",
    "customer_importance_df = spark.read.csv(importance_file_path, header=True, inferSchema=True)\n",
    "\n",
    "print(\"✅ CustomerImportance.csv loaded successfully.\")\n",
    "\n",
    "# ==============================================================================\n",
    "# CHILD CUSTOMER DETECTION (Less than Rs 23 avg, >= 80 transactions)\n",
    "# ==============================================================================\n",
    "patid2_df = raw_transactions_df \\\n",
    "    .groupBy(\"merchant_id\", \"customer_name\", \"customer_id\") \\\n",
    "    .agg(\n",
    "        avg(\"transaction_amount\").alias(\"avg_txn_value\"),\n",
    "        count(\"transaction_id\").alias(\"txn_count\")\n",
    "    ) \\\n",
    "    .where(\"avg_txn_value < 23 AND txn_count >= 80\")\n",
    "\n",
    "patid2_detections = patid2_df.select(\n",
    "    current_timestamp().alias(\"YStartTime(IST)\"),\n",
    "    current_timestamp().alias(\"detectionTime(IST)\"),\n",
    "    lit(\"PatId2\").alias(\"patternId\"),\n",
    "    lit(\"CHILD\").alias(\"ActionType\"),\n",
    "    col(\"customer_name\"),\n",
    "    col(\"merchant_id\")\n",
    ")\n",
    "\n",
    "# Start the streaming query for PatId2\n",
    "patid2_detections.writeStream \\\n",
    "    .format(\"memory\") \\\n",
    "    .queryName(\"patid2_stream\") \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .start()\n",
    "\n",
    "print(\"✅ PatId2 (CHILD) detection stream started.\")\n",
    "\n",
    "# ==============================================================================\n",
    "# DEI-NEEDED DETECTION (Female customers < Male, and > 100)\n",
    "# ==============================================================================\n",
    "unique_customers_per_merchant = raw_transactions_df.select(\"merchant_id\", \"customer_id\", \"gender\").dropDuplicates()\n",
    "\n",
    "patid3_df = unique_customers_per_merchant.groupBy(\"merchant_id\") \\\n",
    "    .agg(\n",
    "        sum(when(col(\"gender\") == 'Male', 1).otherwise(0)).alias(\"male_customers\"),\n",
    "        sum(when(col(\"gender\") == 'Female', 1).otherwise(0)).alias(\"female_customers\")\n",
    "    ) \\\n",
    "    .where(\"female_customers > 100 AND female_customers < male_customers\")\n",
    "\n",
    "patid3_detections = patid3_df.select(\n",
    "    current_timestamp().alias(\"YStartTime(IST)\"),\n",
    "    current_timestamp().alias(\"detectionTime(IST)\"),\n",
    "    lit(\"PatId3\").alias(\"patternId\"),\n",
    "    lit(\"DEI-NEEDED\").alias(\"ActionType\"),\n",
    "    lit(\"\").alias(\"customer_name\"),\n",
    "    col(\"merchant_id\")\n",
    ")\n",
    "\n",
    "# Start the streaming query for PatId3\n",
    "patid3_detections.writeStream \\\n",
    "    .format(\"memory\") \\\n",
    "    .queryName(\"patid3_stream\") \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .start()\n",
    "\n",
    "print(\"✅ PatId3 (DEI-NEEDED) detection stream started.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8572ace0-7722-4980-9401-b95c5c19976d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CustomerImportance.csv loaded and cached.\n✅ PatId1 (UPGRADE) detection stream started with corrected column names.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# ==================================================================================\n",
    "# UPGRADE DETECTION\n",
    "# ==================================================================================\n",
    "\n",
    "# Load the static data\n",
    "importance_file_path = \"/FileStore/tables/CustomerImportance.csv\"\n",
    "customer_importance_df = spark.read.csv(importance_file_path, header=True, inferSchema=True).cache()\n",
    "print(\"✅ CustomerImportance.csv loaded and cached.\")\n",
    "\n",
    "\n",
    "def process_batch_for_patid1(micro_batch_df, batch_id):\n",
    "    if micro_batch_df.rdd.isEmpty():\n",
    "        return\n",
    "\n",
    "    print(f\"--- PatId1: Processing micro-batch {batch_id} ---\")\n",
    "\n",
    "    # Calculate transaction counts and percentile rank\n",
    "    customer_merchant_counts = micro_batch_df.groupBy(\"merchant_id\", \"customer_name\", \"customer_id\", \"transaction_type\") \\\n",
    "        .agg(F.count(\"transaction_id\").alias(\"txn_count\"))\n",
    "    window_spec = Window.partitionBy(\"merchant_id\").orderBy(F.col(\"txn_count\").desc())\n",
    "    percentile_df = customer_merchant_counts.withColumn(\"percentile_rank\", F.cume_dist().over(window_spec))\n",
    "\n",
    "    # Filter for top 1% of customers\n",
    "    top_customers_df = percentile_df.filter(F.col(\"percentile_rank\") <= 0.01)\n",
    "\n",
    "    if top_customers_df.rdd.isEmpty():\n",
    "        return\n",
    "\n",
    "    # Join with customer importance data USING THE CORRECT COLUMN NAMES\n",
    "    join_conditions = [\n",
    "        top_customers_df.customer_id == customer_importance_df.Source,       \n",
    "        top_customers_df.transaction_type == customer_importance_df.typeTrans \n",
    "    ]\n",
    "    upgrades_with_weight = top_customers_df.join(customer_importance_df, join_conditions, \"inner\")\n",
    "\n",
    "    # Filter for customers with bottom 1% weight USING THE CORRECT COLUMN NAME\n",
    "    potential_upgrades = upgrades_with_weight.filter(F.col(\"Weight\") <= 0.01) \n",
    "\n",
    "    if not potential_upgrades.rdd.isEmpty():\n",
    "        final_detections = potential_upgrades.select(\n",
    "            F.current_timestamp().alias(\"YStartTime(IST)\"),\n",
    "            F.current_timestamp().alias(\"detectionTime(IST)\"),\n",
    "            F.lit(\"PatId1\").alias(\"patternId\"),\n",
    "            F.lit(\"UPGRADE\").alias(\"ActionType\"),\n",
    "            F.col(\"customer_name\"),\n",
    "            F.col(\"merchant_id\")\n",
    "        )\n",
    "        final_detections.write.mode(\"append\").saveAsTable(\"patid1_all_detections\")\n",
    "        print(f\"✅ PatId1: SUCCESS! Wrote {final_detections.count()} detections to the table.\")\n",
    "\n",
    "# --- Start the streaming query ---\n",
    "query_patid1 = raw_transactions_df.writeStream \\\n",
    "    .foreachBatch(process_batch_for_patid1) \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .start()\n",
    "\n",
    "print(\"✅ PatId1 (UPGRADE) detection stream started with corrected column names.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2c7d4b5-acd0-43ec-bd5f-e158d5620b84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\nRequirement already satisfied: psycopg2-binary in /local_disk0/.ephemeral_nfs/envs/pythonEnv-1f38e06d-4466-407e-a9ef-6b537cc68121/lib/python3.10/site-packages (2.9.10)\n"
     ]
    }
   ],
   "source": [
    "%pip install psycopg2-binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77f1d62f-5138-4f67-ba4a-d5c4aadbfac5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to connect to PostgreSQL...\n✅✅✅ SUCCESS: Connection to PostgreSQL database was successful!\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import psycopg2\n",
    "\n",
    "# The connection details you provided\n",
    "postgres_conn_details = {\n",
    "    \"host\": \"devdolphines-task-db.c0jy40iesd39.us-east-1.rds.amazonaws.com\",\n",
    "    \"dbname\": \"postgres\",\n",
    "    \"user\": \"postgres\",\n",
    "    \"password\": \"Yogamaya9699\",\n",
    "    \"port\": \"5432\"\n",
    "}\n",
    "\n",
    "print(\"Attempting to connect to PostgreSQL...\")\n",
    "\n",
    "try:\n",
    "    conn = psycopg2.connect(connect_timeout=10, **postgres_conn_details)\n",
    "    print(\"✅✅✅ SUCCESS: Connection to PostgreSQL database was successful!\")\n",
    "    conn.close()\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"❌❌❌ FAILED: Could not connect to the database.\")\n",
    "    print(f\"Error details: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8550e9f9-c321-4e0b-be55-6f269c3746c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import psycopg2\n",
    "from pyspark.sql.window import Window\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# The connection details you have already defined\n",
    "postgres_conn_details = {\n",
    "    \"host\": \"devdolphines-task-db.c0jy40iesd39.us-east-1.rds.amazonaws.com\",\n",
    "    \"dbname\": \"postgres\",\n",
    "    \"user\": \"postgres\",\n",
    "    \"password\": \"Yogamaya9699\",\n",
    "    \"port\": \"5432\"\n",
    "}\n",
    "# Make sure the importance DataFrame is loaded and cached\n",
    "importance_file_path = \"/FileStore/tables/CustomerImportance.csv\"\n",
    "customer_importance_df = spark.read.csv(importance_file_path, header=True, inferSchema=True).cache()\n",
    "\n",
    "\n",
    "def process_patid1_with_postgres(micro_batch_df, batch_id):\n",
    "    if micro_batch_df.rdd.isEmpty():\n",
    "        return\n",
    "\n",
    "    print(f\"--- PatId1: Processing batch {batch_id} ---\")\n",
    "\n",
    "    # 1. Update total transaction counts in PostgreSQL\n",
    "    batch_counts = micro_batch_df.groupBy(\"merchant_id\").count().withColumnRenamed(\"count\", \"new_transactions\")\n",
    "    try:\n",
    "        conn = psycopg2.connect(**postgres_conn_details)\n",
    "        cursor = conn.cursor()\n",
    "        for row in batch_counts.collect():\n",
    "            cursor.execute(\"\"\"\n",
    "                INSERT INTO merchant_transaction_counts (merchant_id, total_transactions)\n",
    "                VALUES (%s, %s) ON CONFLICT (merchant_id) DO UPDATE\n",
    "                SET total_transactions = merchant_transaction_counts.total_transactions + EXCLUDED.total_transactions;\n",
    "            \"\"\", (row[\"merchant_id\"], row[\"new_transactions\"]))\n",
    "        conn.commit()\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error updating PostgreSQL: {e}\")\n",
    "        return\n",
    "\n",
    "    # Read updated counts and filter for merchants with > 50k transactions\n",
    "    pg_url = f\"jdbc:postgresql://{postgres_conn_details['host']}:{postgres_conn_details['port']}/{postgres_conn_details['dbname']}\"\n",
    "    pg_properties = {\"user\": postgres_conn_details[\"user\"], \"password\": postgres_conn_details[\"password\"], \"driver\": \"org.postgresql.Driver\"}\n",
    "    updated_counts_df = spark.read.jdbc(url=pg_url, table=\"merchant_transaction_counts\", properties=pg_properties)\n",
    "    merchants_to_upgrade = updated_counts_df.filter(F.col(\"total_transactions\") > 50000)\n",
    "\n",
    "    if merchants_to_upgrade.rdd.isEmpty():\n",
    "        print(f\"--- PatId1: No merchants have >50k transactions yet. ---\")\n",
    "        return\n",
    "\n",
    "    # Filter the current batch for transactions from these eligible merchants\n",
    "    eligible_transactions_df = micro_batch_df.join(merchants_to_upgrade.select(\"merchant_id\"), \"merchant_id\", \"inner\")\n",
    "    \n",
    "    # Calculate transaction counts for the eligible transactions\n",
    "    customer_merchant_counts = eligible_transactions_df.groupBy(\"merchant_id\", \"customer_name\", \"customer_id\", \"transaction_type\") \\\n",
    "        .agg(F.count(\"transaction_id\").alias(\"txn_count\"))\n",
    "\n",
    "    # Calculate percentile rank\n",
    "    window_spec = Window.partitionBy(\"merchant_id\").orderBy(F.col(\"txn_count\").desc())\n",
    "    percentile_df = customer_merchant_counts.withColumn(\"percentile_rank\", F.cume_dist().over(window_spec))\n",
    "\n",
    "    # Filter for top 1% of customers\n",
    "    top_customers_df = percentile_df.filter(F.col(\"percentile_rank\") <= 0.01)\n",
    "\n",
    "    if top_customers_df.rdd.isEmpty():\n",
    "        print(f\"--- PatId1: No customers in top 1% for eligible merchants in this batch. ---\")\n",
    "        return\n",
    "\n",
    "    # Join with customer importance data using the correct column names from your CSV\n",
    "    join_conditions = [\n",
    "        top_customers_df.customer_id == customer_importance_df.Source,\n",
    "        top_customers_df.transaction_type == customer_importance_df.typeTrans\n",
    "    ]\n",
    "    upgrades_with_weight = top_customers_df.join(customer_importance_df, join_conditions, \"inner\")\n",
    "\n",
    "    # Filter for customers with bottom 1% weight\n",
    "    potential_upgrades = upgrades_with_weight.filter(F.col(\"Weight\") <= 0.01)\n",
    "\n",
    "    if not potential_upgrades.rdd.isEmpty():\n",
    "        # Format the final detections and save them\n",
    "        final_detections = potential_upgrades.select(\n",
    "            F.current_timestamp().alias(\"YStartTime(IST)\"),\n",
    "            F.current_timestamp().alias(\"detectionTime(IST)\"),\n",
    "            F.lit(\"PatId1\").alias(\"patternId\"),\n",
    "            F.lit(\"UPGRADE\").alias(\"ActionType\"),\n",
    "            F.col(\"customer_name\"),\n",
    "            F.col(\"merchant_id\")\n",
    "        )\n",
    "        # For testing, save to a Databricks table.\n",
    "        final_detections.write.mode(\"append\").saveAsTable(\"patid1_all_detections\")\n",
    "        print(f\"✅ PatId1: SUCCESS! Wrote {final_detections.count()} UPGRADE detections to the table.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "524dc025-e03c-4b0d-a097-677777b73aff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ PatId1 stream with full PostgreSQL and pattern logic has started.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "query_patid1 = raw_transactions_df.writeStream \\\n",
    "    .foreachBatch(process_patid1_with_postgres) \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .start()\n",
    "\n",
    "print(\"✅ PatId1 stream with full PostgreSQL and pattern logic has started.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "844d3849-72c2-4303-8f09-5178717578ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Final production stream started. Writing all detections to S3.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =======================================================================\n",
    "#\n",
    "# FINAL SCRIPT: Processing All Patterns and Writing to S3\n",
    "#\n",
    "# =======================================================================\n",
    "\n",
    "S3_OUTPUT_PATH = \"s3a://devdolphines-task-patterns-output-krishna/\" \n",
    "def process_all_patterns_and_save_to_s3(micro_batch_df, batch_id):\n",
    "    \"\"\"\n",
    "    This function processes a micro-batch to find all three patterns\n",
    "    and writes the combined result to a unique file in S3.\n",
    "    \"\"\"\n",
    "    if micro_batch_df.rdd.isEmpty():\n",
    "        return\n",
    "\n",
    "    print(f\"--- Final Processing for batch {batch_id} with {micro_batch_df.count()} records ---\")\n",
    "\n",
    "    # --- Calculate Detections for Each Pattern ---\n",
    "\n",
    "    # PatId2 Logic\n",
    "    patid2_results = micro_batch_df.groupBy(\"merchant_id\", \"customer_name\") \\\n",
    "        .agg(F.avg(\"transaction_amount\").alias(\"avg_txn_value\"), F.count(\"transaction_id\").alias(\"txn_count\")) \\\n",
    "        .where(\"avg_txn_value < 23 AND txn_count >= 80\") \\\n",
    "        .select(\n",
    "            F.lit(\"PatId2\").alias(\"patternId\"),\n",
    "            F.lit(\"CHILD\").alias(\"ActionType\"),\n",
    "            F.col(\"customer_name\"),\n",
    "            F.col(\"merchant_id\")\n",
    "        )\n",
    "\n",
    "    # PatId3 Logic\n",
    "    unique_customers = micro_batch_df.select(\"merchant_id\", \"customer_id\", \"gender\").dropDuplicates()\n",
    "    patid3_results = unique_customers.groupBy(\"merchant_id\") \\\n",
    "        .agg(F.sum(F.when(F.col(\"gender\") == 'Male', 1).otherwise(0)).alias(\"male_customers\"),\n",
    "             F.sum(F.when(F.col(\"gender\") == 'Female', 1).otherwise(0)).alias(\"female_customers\")) \\\n",
    "        .where(\"female_customers > 100 AND female_customers < male_customers\") \\\n",
    "        .select(\n",
    "            F.lit(\"PatId3\").alias(\"patternId\"),\n",
    "            F.lit(\"DEI-NEEDED\").alias(\"ActionType\"),\n",
    "            F.lit(\"\").alias(\"customer_name\"),\n",
    "            F.col(\"merchant_id\")\n",
    "        )\n",
    "    \n",
    "    # --- Combine all detections from this batch ---\n",
    "    all_detections_df = patid2_results.unionByName(patid3_results, allowMissingColumns=True)\n",
    "\n",
    "    # Add timestamp columns\n",
    "    final_detections_df = all_detections_df.withColumn(\"YStartTime(IST)\", F.current_timestamp()) \\\n",
    "                                           .withColumn(\"detectionTime(IST)\", F.current_timestamp())\n",
    "\n",
    "    # Reorder columns to match final format\n",
    "    final_detections_df = final_detections_df.select(\n",
    "        \"YStartTime(IST)\", \"detectionTime(IST)\", \"patternId\", \"ActionType\", \"customerName\", \"MerchantId\"\n",
    "    )\n",
    "\n",
    "    # --- WRITE THE FINAL OUTPUT TO S3 ---\n",
    "    if not final_detections_df.rdd.isEmpty():\n",
    "        detections_count = final_detections_df.count()\n",
    "        print(f\"✅ Found {detections_count} total detections in batch {batch_id}. Writing to S3...\")\n",
    "        \n",
    "        # Generate a unique file name using the batch_id and a timestamp\n",
    "        import time\n",
    "        timestamp_ms = int(time.time() * 1000)\n",
    "        output_file_name = f\"detections_batch_{batch_id}_{timestamp_ms}\"\n",
    "        full_output_path = S3_OUTPUT_PATH + output_file_name\n",
    "        \n",
    "        # Write the combined DataFrame to a single CSV file in your output bucket\n",
    "        final_detections_df.coalesce(1).write.mode(\"append\").option(\"header\", \"true\").csv(full_output_path)\n",
    "        print(f\"Successfully wrote detections to {full_output_path}\")\n",
    "\n",
    "# --- Start the Final Production Stream ---\n",
    "query = raw_transactions_df.writeStream \\\n",
    "    .foreachBatch(process_all_patterns_and_save_to_s3) \\\n",
    "    .start()\n",
    "\n",
    "print(\"✅ Final production stream started. Writing all detections to S3.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2243253776581987,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Mechanism_Y",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}